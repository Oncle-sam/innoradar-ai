import streamlit as st
import google.generativeai as genai

# --- CONFIGURATION DE L'IA ---
# On r√©cup√®re la cl√© de mani√®re s√©curis√©e (on verra cela √† l'√©tape 5)
api_key = st.secrets.get("GEMINI_API_KEY")

if not api_key:
    st.error("Cl√© API manquante. Veuillez la configurer dans les secrets.")
    st.stop()

genai.configure(api_key=api_key)

# --- CONFIGURATION DE LA PERSONNALIT√â ---
SYSTEM_PROMPT = """
Tu es Innoradar AI, un expert en analyse d'innovation. 
Ton r√¥le est d'analyser les projets tech et sportifs selon les crit√®res suivants :
1. Degr√© de rupture technologique.
2. Viabilit√© sur le march√©.
3. Impact potentiel.
R√©ponds toujours de mani√®re structur√©e et professionnelle.
"""

model = genai.GenerativeModel(
    model_name='gemini-1.5-flash',
    system_instruction=SYSTEM_PROMPT
)

# --- INTERFACE UTILISATEUR ---
st.set_page_config(page_title="Innoradar", page_icon="üöÄ")

st.title("üöÄ Mon Application Gemini B√™ta")
st.write("Bienvenue dans cette version test. Posez votre question ci-dessous.")

# Historique de chat (pour le c√¥t√© interactif)
if "messages" not in st.session_state:
    st.session_state.messages = []

# Affichage des messages pr√©c√©dents
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Zone de saisie
if prompt := st.chat_input("Dites quelque chose..."):
    # Afficher le message utilisateur
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # R√©ponse de l'IA
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        try:
            # Appel √† Gemini
            response = model.generate_content(prompt)
            full_response = response.text
            message_placeholder.markdown(full_response)
        except Exception as e:
            st.error(f"Erreur : {e}")
            
    st.session_state.messages.append({"role": "assistant", "content": full_response})